# Example config file to process Level 2 MC i3-files
#
# Directory structure:
#       At prompt of create_job_files.py a data_folder
#       will be asked for in which the files are to be
#       saved
#
#   Files are then stored as:
#
#   data_folder:
#       processing:
#           out_dir_pattern:
#               jobs: (job files are stored here)
#               logs: (log files are stored here)
#
#       out_dir_pattern:
#
#               run_folder:
#                       out_file_pattern + '.' + i3_ending
#                       out_file_pattern + '.hdf5'
#
#
#               Where the run folder is given by:
#               run_folder = folder_pattern.format(
#                 folder_num = folder_offset + run_number // n_jobs_per_folder
#                 folder_num_pre_offset = run_number // n_jobs_per_folder
#               )
#
#       The following variables are computed and can be used in input/output patterns
#       folder_num = folder_offset + run_number // n_jobs_per_folder
#       folder_num_pre_offset = run_number // n_jobs_per_folder
#

#------------------------------
# General job submission config
#------------------------------

keep_crashed_files: False

resources:
        # If gpus == 1 this will be run on a GPU with
        gpus: 1
        cpus: 1
        memory: 3gb
        # has_ssse3: True
        # has_avx2: True

dagman_max_jobs: 5000
dagman_submits_interval: 500
dagman_scan_interval: 1
dagman_submit_delay: 0

# If true, the input files will first be checked for corruption.
# Note that this will take a while, since each input file has to be
# iterated through. You generally only want to set this to true if you
# are merging a number of input files of which some are known to be corrupt.
exclude_corrupted_input_files: False

#------------------------------
# Define Datasets to process
#------------------------------

#------
# common settings shared by all datasets
#------
i3_ending: 'i3.zst'
in_file_pattern: '/data/sim/IceCube/2020/filtered/level2/neutrino-generator/{dataset_number}/{folder_pattern}/Level2_{year}_{flavor}.{dataset_number:06d}.{run_number:06d}.i3.zst'
out_dir_pattern: '{data_type}/{dataset_number}/'
out_file_pattern: '{data_type}_{dataset_number}_{run_number:08d}'
folder_pattern: '{folder_num_pre_offset:04d}000-{folder_num_pre_offset:04d}999'
folder_offset: 0
n_jobs_per_folder: 1000
gcd: /cvmfs/icecube.opensciencegrid.org/data/GCD/GeoCalibDetectorStatus_2020.Run134142.Pass2_V0.i3.gz
#------


datasets:

    # ------------
    # Spice BFR-v1
    # ------------
    NuGen_NuMu_bfrv1:
        cycler:
                dataset_number: [21535]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuMu'
        year: 'IC86.2020'

    NuGen_NuTau_bfrv1:
        cycler:
                dataset_number: [21536]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuTau'
        year: 'IC86.2020'

    NuGen_NuE_bfrv1:
        cycler:
                dataset_number: [21537]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuE'
        year: 'IC86.2020'


# -------------------------------------------------------------
# Define environment information shared across processing steps
# -------------------------------------------------------------
job_template: job_templates/cvmfs_python.sh
script_name: general_i3_processing.py
python_user_base_cpu: /data/ana/PointSource/DNNCascade/utils/virtualenvs/version-0.0/py3-v4.1.1_tensorflow2.3
python_user_base_gpu: /data/ana/PointSource/DNNCascade/utils/virtualenvs/version-0.0/py3-v4.1.1_tensorflow2.3
cuda_home: /data/ana/PointSource/DNNCascade/utils/cuda/cuda-10.1

# add optional additions to the LD_LIBRARY_PATH
# Note: '{ld_library_path_prepends}' is the default which does not add anything
ld_library_path_prepends: '{ld_library_path_prepends}'

# Defines environment variables that are set from python
set_env_vars_from_python: {
    'TF_DETERMINISTIC_OPS': '1',
}

#-----------------------------------------------
# Define I3Traysegments for each processing step
#-----------------------------------------------

# a list of processing steps. Each processing step contains
# information on the python and cvmfs environment as well as
# a list of I3TraySegments/Modules that will be added to the I3Tray.
# Any options defined in these nested dictionaries will supersede the
# ones defined globally in this config.
processing_steps: [
    {
        cvmfs_python: py3-v4.1.1
        icetray_metaproject: combo/V01-00-00

        # define a list of tray segments to run
        tray_segments: [
            {
                ModuleClass: '',
                ModuleKwargs: {},
            }
        ]
    }
    {
        cvmfs_python: py3-v4.2.1
        icetray_metaproject: combo/V01-00-00

        # define a list of tray segments to run
        tray_segments: [
        ]
    }
]

#--------------------
# File output options
#--------------------

# write output as i3 files via the I3Writer
write_i3: True
write_i3_kwargs: {

    # only write these stream types,
    # i.e ['Q', 'P', 'I', 'S', 'M', 'm', 'W', 'X']
    'i3_streams': ['Q', 'P', 'I', 'S', 'M', 'm', 'W', 'X']
}

# write output as hdf5 files via the I3HDFWriter
write_hdf5: False
write_hdf5_kwargs: {

    # sub event streams to write
    'SubEventStreams': ['InIceSplit']

    # HDF keys to write (in addition to the ones in tray.context['HDF_keys'])
    # Note: added tray segments should add outputs that should be written
    # to hdf5 to the tray context.
    'Keys': [],
}
